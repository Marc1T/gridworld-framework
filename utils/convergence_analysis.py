"""
Outils d'analyse de convergence et de sensibilitÃ© pour les agents RL.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from gridworld_framework.agents.base_agent import BaseAgent
from gridworld_framework.core.gridworld_env import GridWorldEnv


def analyze_convergence(agent: BaseAgent, 
                       metric: str = 'Q',
                       window_size: int = 10) -> Dict[str, Any]:
    """
    Analyse la convergence d'un agent.
    
    Args:
        agent: Agent entraÃ®nÃ©
        metric: MÃ©trique Ã  analyser ('Q', 'V', 'rewards')
        window_size: Taille de la fenÃªtre pour dÃ©tecter la convergence
        
    Returns:
        Dictionnaire avec les mÃ©triques de convergence
    """
    if metric == 'Q' and agent.convergence_history:
        values = agent.convergence_history
    elif metric == 'V' and agent.v_history:
        values = [np.max(v) for v in agent.v_history]
    elif metric == 'rewards' and agent.rewards_history:
        values = agent.rewards_history
    else:
        return {'error': 'Pas de donnÃ©es disponibles'}
    
    if len(values) < window_size:
        return {'error': 'Pas assez de donnÃ©es'}
    
    # Calculer les diffÃ©rences
    diffs = np.abs(np.diff(values))
    
    # Moyenne mobile des diffÃ©rences
    moving_avg = np.convolve(diffs, np.ones(window_size)/window_size, mode='valid')
    
    # Trouver le point de convergence (quand les changements deviennent < 1% du max)
    threshold = np.max(diffs) * 0.01
    convergence_points = np.where(moving_avg < threshold)[0]
    
    if len(convergence_points) > 0:
        convergence_episode = convergence_points[0] + window_size
        converged = True
    else:
        convergence_episode = len(values)
        converged = False
    
    # Vitesse de convergence (pente moyenne avant convergence)
    if convergence_episode > 1:
        convergence_speed = (values[convergence_episode-1] - values[0]) / convergence_episode
    else:
        convergence_speed = 0
    
    # StabilitÃ© aprÃ¨s convergence
    if converged and convergence_episode < len(values) - window_size:
        post_convergence = values[convergence_episode:]
        stability = np.std(post_convergence) / (np.mean(np.abs(post_convergence)) + 1e-8)
    else:
        stability = np.inf
    
    return {
        'converged': converged,
        'convergence_episode': convergence_episode,
        'convergence_speed': convergence_speed,
        'stability': stability,
        'final_value': values[-1],
        'max_value': np.max(values),
        'total_episodes': len(values),
        'convergence_ratio': convergence_episode / len(values) if len(values) > 0 else 1.0
    }


def test_gridworld_scalability(agent_class, 
                               grid_sizes: List[Tuple[int, int]] = None,
                               n_episodes: int = 1000,
                               **agent_kwargs) -> Dict[str, List]:
    """
    Test la scalabilitÃ© d'un algorithme sur diffÃ©rentes tailles de grille.
    
    Args:
        agent_class: Classe de l'agent Ã  tester
        grid_sizes: Liste des tailles de grille Ã  tester
        n_episodes: Nombre d'Ã©pisodes par test
        **agent_kwargs: Arguments pour l'agent
        
    Returns:
        RÃ©sultats des tests de scalabilitÃ©
    """
    if grid_sizes is None:
        grid_sizes = [(4, 4), (5, 5), (6, 6), (8, 8), (10, 10)]
    
    results = {
        'grid_sizes': [],
        'n_states': [],
        'convergence_episodes': [],
        'final_rewards': [],
        'success_rates': [],
        'training_times': [],
        'convergence_speeds': []
    }
    
    for grid_size in grid_sizes:
        print(f"\nğŸ”¬ Test sur grille {grid_size[0]}x{grid_size[1]}...")
        
        n_states = grid_size[0] * grid_size[1]
        goal_state = n_states - 1
        
        # CrÃ©er l'environnement
        env = GridWorldEnv(
            grid_shape=grid_size,
            initial_state=0,
            goal_state=goal_state
        )
        
        # CrÃ©er et entraÃ®ner l'agent
        agent = agent_class(env, **agent_kwargs)
        
        import time
        start_time = time.time()
        history = agent.train(n_episodes=n_episodes, verbose=False)
        training_time = time.time() - start_time
        
        # Analyser la convergence
        conv_analysis = analyze_convergence(agent, metric='Q')
        
        # Ã‰valuer l'agent
        eval_results = agent.evaluate(n_episodes=100)
        
        # Stocker les rÃ©sultats
        results['grid_sizes'].append(f"{grid_size[0]}x{grid_size[1]}")
        results['n_states'].append(n_states)
        results['convergence_episodes'].append(conv_analysis.get('convergence_episode', n_episodes))
        results['final_rewards'].append(eval_results['mean_reward'])
        results['success_rates'].append(eval_results['success_rate'])
        results['training_times'].append(training_time)
        results['convergence_speeds'].append(conv_analysis.get('convergence_speed', 0))
        
        print(f"  âœ“ Ã‰tats: {n_states}, Convergence: {conv_analysis.get('convergence_episode', 'N/A')}, "
              f"SuccÃ¨s: {eval_results['success_rate']:.1f}%, Temps: {training_time:.2f}s")
    
    return results


def test_hyperparameter_sensitivity(agent_class,
                                   env,
                                   param_name: str,
                                   param_values: List[float],
                                   n_episodes: int = 1000,
                                   n_trials: int = 5,
                                   **fixed_kwargs) -> Dict[str, List]:
    """
    Test la sensibilitÃ© d'un agent Ã  un hyperparamÃ¨tre.
    
    Args:
        agent_class: Classe de l'agent
        env: Environnement
        param_name: Nom de l'hyperparamÃ¨tre Ã  tester
        param_values: Valeurs Ã  tester
        n_episodes: Nombre d'Ã©pisodes par test
        n_trials: Nombre d'essais par valeur (pour moyenne)
        **fixed_kwargs: Autres paramÃ¨tres fixes
        
    Returns:
        RÃ©sultats des tests de sensibilitÃ©
    """
    results = {
        'param_values': param_values,
        'mean_rewards': [],
        'std_rewards': [],
        'mean_success_rates': [],
        'std_success_rates': [],
        'mean_convergence': [],
        'std_convergence': []
    }
    
    for param_value in param_values:
        print(f"\nğŸ§ª Test {param_name} = {param_value}...")
        
        trial_rewards = []
        trial_success = []
        trial_convergence = []
        
        for trial in range(n_trials):
            # CrÃ©er l'agent avec le paramÃ¨tre testÃ©
            kwargs = fixed_kwargs.copy()
            kwargs[param_name] = param_value
            
            agent = agent_class(env, **kwargs)
            
            # EntraÃ®ner
            history = agent.train(n_episodes=n_episodes, verbose=False)
            
            # Ã‰valuer
            eval_results = agent.evaluate(n_episodes=100)
            
            # Convergence
            conv_analysis = analyze_convergence(agent, metric='Q')
            
            trial_rewards.append(eval_results['mean_reward'])
            trial_success.append(eval_results['success_rate'])
            trial_convergence.append(conv_analysis.get('convergence_episode', n_episodes))
        
        # Moyennes et Ã©carts-types
        results['mean_rewards'].append(np.mean(trial_rewards))
        results['std_rewards'].append(np.std(trial_rewards))
        results['mean_success_rates'].append(np.mean(trial_success))
        results['std_success_rates'].append(np.std(trial_success))
        results['mean_convergence'].append(np.mean(trial_convergence))
        results['std_convergence'].append(np.std(trial_convergence))
        
        print(f"  âœ“ Reward: {np.mean(trial_rewards):.2f} Â± {np.std(trial_rewards):.2f}, "
              f"SuccÃ¨s: {np.mean(trial_success):.1f}% Â± {np.std(trial_success):.1f}%")
    
    return results


def plot_convergence_comparison(agents_results: Dict[str, Dict],
                               figsize: Tuple[int, int] = (15, 5)) -> plt.Figure:
    """
    Compare la convergence de plusieurs agents.
    
    Args:
        agents_results: Dict {nom_agent: historique}
        figsize: Taille de la figure
        
    Returns:
        Figure matplotlib
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    # 1. Courbes de rÃ©compenses
    for name, history in agents_results.items():
        if 'rewards' in history:
            rewards = history['rewards']
            # Moyenne mobile
            window = max(1, len(rewards) // 20)
            if len(rewards) >= window:
                moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
                axes[0].plot(range(window-1, len(rewards)), moving_avg, 
                           label=name, linewidth=2)
    
    axes[0].set_xlabel('Ã‰pisode')
    axes[0].set_ylabel('RÃ©compense moyenne')
    axes[0].set_title('Convergence des rÃ©compenses')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # 2. Convergence de Q
    for name, history in agents_results.items():
        if 'convergence' in history:
            axes[1].plot(history['convergence'], label=name, linewidth=2)
    
    axes[1].set_xlabel('Ã‰pisode')
    axes[1].set_ylabel('Q max')
    axes[1].set_title('Convergence de Q(s,a)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # 3. Taux de succÃ¨s cumulÃ©
    for name, history in agents_results.items():
        if 'success_rate' in history:
            success = history['success_rate']
            # Taux de succÃ¨s cumulÃ©
            cumulative_success = np.cumsum(success) / np.arange(1, len(success) + 1) * 100
            axes[2].plot(cumulative_success, label=name, linewidth=2)
    
    axes[2].set_xlabel('Ã‰pisode')
    axes[2].set_ylabel('Taux de succÃ¨s (%)')
    axes[2].set_title('Taux de succÃ¨s cumulÃ©')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    axes[2].set_ylim([0, 105])
    
    plt.tight_layout()
    return fig


def plot_scalability_results(results: Dict[str, List],
                            figsize: Tuple[int, int] = (15, 5)) -> plt.Figure:
    """
    Visualise les rÃ©sultats de scalabilitÃ©.
    
    Args:
        results: RÃ©sultats de test_gridworld_scalability
        figsize: Taille de la figure
        
    Returns:
        Figure matplotlib
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    n_states = results['n_states']
    
    # 1. Ã‰pisodes de convergence vs taille
    axes[0].plot(n_states, results['convergence_episodes'], 
                marker='o', linewidth=2, markersize=8)
    axes[0].set_xlabel('Nombre d\'Ã©tats')
    axes[0].set_ylabel('Ã‰pisodes de convergence')
    axes[0].set_title('ScalabilitÃ© de la convergence')
    axes[0].grid(True, alpha=0.3)
    
    # 2. Taux de succÃ¨s vs taille
    axes[1].plot(n_states, results['success_rates'], 
                marker='s', linewidth=2, markersize=8, color='green')
    axes[1].set_xlabel('Nombre d\'Ã©tats')
    axes[1].set_ylabel('Taux de succÃ¨s (%)')
    axes[1].set_title('Performance vs taille de grille')
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim([0, 105])
    
    # 3. Temps d'entraÃ®nement vs taille
    axes[2].plot(n_states, results['training_times'], 
                marker='^', linewidth=2, markersize=8, color='red')
    axes[2].set_xlabel('Nombre d\'Ã©tats')
    axes[2].set_ylabel('Temps (secondes)')
    axes[2].set_title('Temps d\'entraÃ®nement')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig


def plot_sensitivity_analysis(results: Dict[str, List],
                             param_name: str,
                             figsize: Tuple[int, int] = (15, 5)) -> plt.Figure:
    """
    Visualise l'analyse de sensibilitÃ©.
    
    Args:
        results: RÃ©sultats de test_hyperparameter_sensitivity
        param_name: Nom du paramÃ¨tre testÃ©
        figsize: Taille de la figure
        
    Returns:
        Figure matplotlib
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    param_values = results['param_values']
    
    # 1. RÃ©compenses
    axes[0].errorbar(param_values, results['mean_rewards'], 
                    yerr=results['std_rewards'],
                    marker='o', linewidth=2, markersize=8, capsize=5)
    axes[0].set_xlabel(param_name)
    axes[0].set_ylabel('RÃ©compense moyenne')
    axes[0].set_title(f'SensibilitÃ©: RÃ©compenses vs {param_name}')
    axes[0].grid(True, alpha=0.3)
    
    # 2. Taux de succÃ¨s
    axes[1].errorbar(param_values, results['mean_success_rates'], 
                    yerr=results['std_success_rates'],
                    marker='s', linewidth=2, markersize=8, capsize=5, color='green')
    axes[1].set_xlabel(param_name)
    axes[1].set_ylabel('Taux de succÃ¨s (%)')
    axes[1].set_title(f'SensibilitÃ©: SuccÃ¨s vs {param_name}')
    axes[1].grid(True, alpha=0.3)
    
    # 3. Convergence
    axes[2].errorbar(param_values, results['mean_convergence'], 
                    yerr=results['std_convergence'],
                    marker='^', linewidth=2, markersize=8, capsize=5, color='purple')
    axes[2].set_xlabel(param_name)
    axes[2].set_ylabel('Ã‰pisodes de convergence')
    axes[2].set_title(f'SensibilitÃ©: Convergence vs {param_name}')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig


def generate_convergence_report(agent: BaseAgent,
                               save_path: Optional[str] = None) -> str:
    """
    GÃ©nÃ¨re un rapport dÃ©taillÃ© de convergence.
    
    Args:
        agent: Agent entraÃ®nÃ©
        save_path: Chemin pour sauvegarder le rapport (optionnel)
        
    Returns:
        Rapport sous forme de string
    """
    # Analyse de convergence
    conv_q = analyze_convergence(agent, metric='Q')
    conv_rewards = analyze_convergence(agent, metric='rewards')
    
    # GÃ©nÃ©rer le rapport
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        RAPPORT D'ANALYSE DE CONVERGENCE                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Agent: {agent.name}
Total d'Ã©pisodes: {agent.total_episodes}
Total de steps: {agent.total_steps}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CONVERGENCE DE Q(s,a)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ConvergÃ©: {'âœ“ OUI' if conv_q.get('converged') else 'âœ— NON'}
Ã‰pisode de convergence: {conv_q.get('convergence_episode', 'N/A')}
Ratio de convergence: {conv_q.get('convergence_ratio', 0)*100:.1f}%
Vitesse de convergence: {conv_q.get('convergence_speed', 0):.6f}
StabilitÃ© post-convergence: {conv_q.get('stability', np.inf):.6f}
Valeur finale de Q max: {conv_q.get('final_value', 0):.4f}
Valeur maximale de Q: {conv_q.get('max_value', 0):.4f}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CONVERGENCE DES RÃ‰COMPENSES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ConvergÃ©: {'âœ“ OUI' if conv_rewards.get('converged') else 'âœ— NON'}
Ã‰pisode de convergence: {conv_rewards.get('convergence_episode', 'N/A')}
Ratio de convergence: {conv_rewards.get('convergence_ratio', 0)*100:.1f}%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PERFORMANCE GLOBALE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
    
    if agent.rewards_history:
        last_100 = agent.rewards_history[-100:]
        report += f"RÃ©compense moyenne (100 derniers): {np.mean(last_100):.2f} Â± {np.std(last_100):.2f}\n"
    
    if agent.success_history:
        last_100_success = agent.success_history[-100:]
        report += f"Taux de succÃ¨s (100 derniers): {np.mean(last_100_success)*100:.1f}%\n"
    
    if agent.episode_lengths:
        last_100_lengths = agent.episode_lengths[-100:]
        report += f"Longueur moyenne d'Ã©pisode: {np.mean(last_100_lengths):.1f} Â± {np.std(last_100_lengths):.1f}\n"
    
    report += "\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
    
    # Sauvegarder si demandÃ©
    if save_path:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ“ Rapport sauvegardÃ©: {save_path}")
    
    return report