"""
Exemple complet d'utilisation du framework GridWorld.

Ce script montre comment:
1. Cr√©er un environnement
2. Entra√Æner diff√©rents agents
3. Visualiser les r√©sultats
4. Analyser la convergence
5. Tester la scalabilit√©
6. Faire des analyses de sensibilit√©
"""

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Imports du framework
from gridworld_framework.core.gridworld_env import GridWorldEnv
from gridworld_framework.agents.q_learning import QLearningAgent
from gridworld_framework.agents.value_iteration import ValueIterationAgent
from gridworld_framework.agents.policy_iteration import PolicyIterationAgent
from gridworld_framework.agents.monte_carlo import MonteCarloAgent

from gridworld_framework.utils.visualization import (
    plot_learning_curve,
    plot_value_function,
    plot_policy,
    plot_q_function
)

from gridworld_framework.utils.convergence_analysis import (
    analyze_convergence,
    test_gridworld_scalability,
    test_hyperparameter_sensitivity,
    plot_convergence_comparison,
    plot_scalability_results,
    plot_sensitivity_analysis,
    generate_convergence_report
)


def example_1_basic_training():
    """Exemple 1: Entra√Ænement basique d'un agent Q-Learning."""
    print("\n" + "="*60)
    print("EXEMPLE 1: Entra√Ænement basique de Q-Learning")
    print("="*60)
    
    # Cr√©er l'environnement
    env = GridWorldEnv(
        grid_shape=(5, 5),
        initial_state=0,
        goal_state=24,
        obstacles=[7, 12, 13]
    )
    
    # Cr√©er l'agent
    agent = QLearningAgent(
        env,
        gamma=0.99,
        learning_rate=0.1,
        epsilon=1.0,
        epsilon_decay=0.995
    )
    
    # Entra√Æner
    print("\nüöÄ D√©but de l'entra√Ænement...")
    history = agent.train(n_episodes=1000, verbose=True)
    
    # √âvaluer
    print("\nüìä √âvaluation de l'agent...")
    eval_results = agent.evaluate(n_episodes=100)
    print(f"R√©compense moyenne: {eval_results['mean_reward']:.2f} ¬± {eval_results['std_reward']:.2f}")
    # print(f"Taux de succ√®s: {eval_results['success_rate']:.1f}%")
    
    # Visualiser
    print("\nüìà Cr√©ation des visualisations...")
    
    # Courbes d'apprentissage
    fig1 = plot_learning_curve(history, title="Q-Learning - Courbe d'apprentissage")
    plt.savefig("results/plots/qlearning_learning_curve.png", dpi=150, bbox_inches='tight')
    
    # Fonction de valeur
    fig2 = plot_value_function(agent.V, env.grid_shape)
    plt.savefig("results/plots/qlearning_value_function.png", dpi=150, bbox_inches='tight')
    
    # Politique
    fig3 = plot_policy(agent.policy, env.grid_shape, action_names=['‚Üë', '‚Üí', '‚Üì', '‚Üê'])
    plt.savefig("results/plots/qlearning_policy.png", dpi=150, bbox_inches='tight')
    
    # Fonction Q
    fig4 = plot_q_function(agent.Q, env.grid_shape, action_names=['‚Üë', '‚Üí', '‚Üì', '‚Üê'])
    plt.savefig("results/plots/qlearning_q_function.png", dpi=150, bbox_inches='tight')
    
    # Sauvegarder l'agent
    agent.save("results/checkpoints/qlearning_final.npz")
    
    print("‚úì Exemple 1 termin√©!")
    plt.show()


def example_2_compare_algorithms():
    """Exemple 2: Comparer plusieurs algorithmes."""
    print("\n" + "="*60)
    print("EXEMPLE 2: Comparaison d'algorithmes")
    print("="*60)
    
    # Cr√©er l'environnement
    env = GridWorldEnv(grid_shape=(4, 4), initial_state=0, goal_state=15)
    
    agents = {}
    histories = {}
    
    # 1. Value Iteration
    print("\nüìö Entra√Ænement: Value Iteration")
    vi_agent = ValueIterationAgent(env, gamma=0.99)
    histories['Value Iteration'] = vi_agent.train(verbose=False)
    agents['Value Iteration'] = vi_agent
    
    # 2. Policy Iteration
    print("üìö Entra√Ænement: Policy Iteration")
    pi_agent = PolicyIterationAgent(env, gamma=0.99)
    histories['Policy Iteration'] = pi_agent.train(verbose=False)
    agents['Policy Iteration'] = pi_agent
    
    # 3. Q-Learning
    print("üìö Entra√Ænement: Q-Learning")
    q_agent = QLearningAgent(env, gamma=0.99, learning_rate=0.1)
    histories['Q-Learning'] = q_agent.train(n_episodes=500, verbose=False)
    agents['Q-Learning'] = q_agent
    
    # 4. Monte Carlo
    print("üìö Entra√Ænement: Monte Carlo")
    mc_agent = MonteCarloAgent(env, gamma=0.99)
    histories['Monte Carlo'] = mc_agent.train(n_episodes=500, verbose=False)
    agents['Monte Carlo'] = mc_agent
    
    # Comparer
    print("\nüìä √âvaluation des agents...")
    for name, agent in agents.items():
        results = agent.evaluate(n_episodes=100)
        # print(f"{name:20s} - Succ√®s: {results['success_rate']:5.1f}%, "
        #       f"Reward: {results['mean_reward']:6.2f}")
    
    # Visualiser la comparaison
    fig = plot_convergence_comparison(histories, figsize=(18, 5))
    plt.savefig("results/plots/algorithms_comparison.png", dpi=150, bbox_inches='tight')
    
    print("‚úì Exemple 2 termin√©!")
    plt.show()


def example_3_scalability_test():
    """Exemple 3: Test de scalabilit√©."""
    print("\n" + "="*60)
    print("EXEMPLE 3: Test de scalabilit√© de Q-Learning")
    print("="*60)
    
    # Tester sur diff√©rentes tailles
    results = test_gridworld_scalability(
        agent_class=QLearningAgent,
        grid_sizes=[(4,4), (5,5), (6,6), (8,8)],
        n_episodes=500,
        gamma=0.99,
        learning_rate=0.1,
        epsilon_decay=0.995
    )
    
    # Visualiser
    fig = plot_scalability_results(results)
    plt.savefig("results/plots/scalability_analysis.png", dpi=150, bbox_inches='tight')
    
    print("\nüìä R√©sum√© de la scalabilit√©:")
    for i, size in enumerate(results['grid_sizes']):
        print(f"Grille {size}: {results['n_states'][i]} √©tats, "
              f"Convergence: {results['convergence_episodes'][i]} √©pisodes, "
              f"Succ√®s: {results['success_rates'][i]:.1f}%")
    
    print("‚úì Exemple 3 termin√©!")
    plt.show()


def example_4_sensitivity_analysis():
    """Exemple 4: Analyse de sensibilit√©."""
    print("\n" + "="*60)
    print("EXEMPLE 4: Analyse de sensibilit√© du learning rate")
    print("="*60)
    
    # Cr√©er l'environnement
    env = GridWorldEnv(grid_shape=(5, 5), initial_state=0, goal_state=24)
    
    # Tester diff√©rents learning rates
    learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9]
    
    results = test_hyperparameter_sensitivity(
        agent_class=QLearningAgent,
        env=env,
        param_name='learning_rate',
        param_values=learning_rates,
        n_episodes=500,
        n_trials=3,  # 3 essais par valeur
        gamma=0.99,
        epsilon_decay=0.995
    )
    
    # Visualiser
    fig = plot_sensitivity_analysis(results, param_name='Learning Rate')
    plt.savefig("results/plots/sensitivity_learning_rate.png", dpi=150, bbox_inches='tight')
    
    print("\nüìä Meilleur learning rate:")
    best_idx = np.argmax(results['mean_success_rates'])
    print(f"Œ± = {learning_rates[best_idx]} "
          f"(Succ√®s: {results['mean_success_rates'][best_idx]:.1f}%)")
    
    print("‚úì Exemple 4 termin√©!")
    plt.show()


def example_5_detailed_convergence_analysis():
    """Exemple 5: Analyse d√©taill√©e de convergence."""
    print("\n" + "="*60)
    print("EXEMPLE 5: Analyse d√©taill√©e de convergence")
    print("="*60)
    
    # Cr√©er et entra√Æner l'agent
    env = GridWorldEnv(grid_shape=(6, 6), initial_state=0, goal_state=35)
    agent = QLearningAgent(env, gamma=0.99, learning_rate=0.1)
    
    print("\nüöÄ Entra√Ænement...")
    agent.train(n_episodes=1000, verbose=False)
    
    # Analyser la convergence
    print("\nüî¨ Analyse de convergence...")
    conv_analysis = analyze_convergence(agent, metric='Q')
    
    print(f"\nR√©sultats:")
    print(f"  Converg√©: {'‚úì OUI' if conv_analysis['converged'] else '‚úó NON'}")
    print(f"  √âpisode de convergence: {conv_analysis['convergence_episode']}")
    print(f"  Vitesse: {conv_analysis['convergence_speed']:.6f}")
    print(f"  Stabilit√©: {conv_analysis['stability']:.6f}")
    
    # G√©n√©rer un rapport complet
    report = generate_convergence_report(agent, save_path="results/convergence_report.txt")
    print(report)
    
    print("‚úì Exemple 5 termin√©!")


def example_6_checkpoint_and_resume():
    """Exemple 6: Sauvegarde et reprise d'entra√Ænement."""
    print("\n" + "="*60)
    print("EXEMPLE 6: Checkpoints et reprise")
    print("="*60)
    
    env = GridWorldEnv(grid_shape=(5, 5), initial_state=0, goal_state=24)
    
    # Entra√Æner partiellement
    print("\nüöÄ Entra√Ænement phase 1 (500 √©pisodes)...")
    agent = QLearningAgent(env, gamma=0.99, learning_rate=0.1)
    agent.train(n_episodes=500, verbose=False)
    
    # Sauvegarder
    agent.save("results/checkpoints/qlearning_checkpoint_500.npz")
    print(f"‚úì Agent sauvegard√© apr√®s {agent.total_episodes} √©pisodes")
    
    # √âvaluation interm√©diaire
    eval1 = agent.evaluate(n_episodes=100)
    print(f"Performance phase 1: Succ√®s = {eval1['success_rate']:.1f}%")
    
    # Charger et continuer
    print("\nüîÑ Reprise de l'entra√Ænement...")
    agent2 = QLearningAgent(env, gamma=0.99, learning_rate=0.1)
    agent2.load("results/checkpoints/qlearning_checkpoint_500.npz")
    
    print(f"‚úì Agent charg√©: {agent2.total_episodes} √©pisodes d√©j√† effectu√©s")
    
    # Continuer l'entra√Ænement
    print("\nüöÄ Entra√Ænement phase 2 (500 √©pisodes suppl√©mentaires)...")
    agent2.train(n_episodes=500, verbose=False)
    
    # √âvaluation finale
    eval2 = agent2.evaluate(n_episodes=100)
    print(f"Performance phase 2: Succ√®s = {eval2['success_rate']:.1f}%")
    print(f"Am√©lioration: {eval2['success_rate'] - eval1['success_rate']:+.1f}%")
    
    print("‚úì Exemple 6 termin√©!")


def example_7_stochastic_environment():
    """Exemple 7: Environnement stochastique."""
    print("\n" + "="*60)
    print("EXEMPLE 7: Environnement stochastique")
    print("="*60)
    
    # Environnement d√©terministe
    env_det = GridWorldEnv(
        grid_shape=(5, 5),
        initial_state=0,
        goal_state=24,
        stochastic=False
    )
    
    # Environnement stochastique (10% de bruit)
    env_stoch = GridWorldEnv(
        grid_shape=(5, 5),
        initial_state=0,
        goal_state=24,
        stochastic=True,
        noise=0.1
    )
    
    results = {}
    
    # Entra√Æner sur d√©terministe
    print("\nüéØ Entra√Ænement: Environnement d√©terministe")
    agent_det = QLearningAgent(env_det, gamma=0.99, learning_rate=0.1)
    history_det = agent_det.train(n_episodes=500, verbose=False)
    results['D√©terministe'] = history_det
    eval_det = agent_det.evaluate(n_episodes=100)
    
    # Entra√Æner sur stochastique
    print("üé≤ Entra√Ænement: Environnement stochastique")
    agent_stoch = QLearningAgent(env_stoch, gamma=0.99, learning_rate=0.1)
    history_stoch = agent_stoch.train(n_episodes=500, verbose=False)
    results['Stochastique'] = history_stoch
    eval_stoch = agent_stoch.evaluate(n_episodes=100)
    
    # Comparer
    print("\nüìä Comparaison:")
    print(f"D√©terministe  - Succ√®s: {eval_det['success_rate']:.1f}%, "
          f"Reward: {eval_det['mean_reward']:.2f}")
    print(f"Stochastique  - Succ√®s: {eval_stoch['success_rate']:.1f}%, "
          f"Reward: {eval_stoch['mean_reward']:.2f}")
    
    # Visualiser
    fig = plot_convergence_comparison(results)
    plt.savefig("results/plots/stochastic_comparison.png", dpi=150, bbox_inches='tight')
    
    print("‚úì Exemple 7 termin√©!")
    plt.show()


def run_all_examples():
    """Ex√©cute tous les exemples."""
    # Cr√©er les dossiers n√©cessaires
    Path("results/plots").mkdir(parents=True, exist_ok=True)
    Path("results/checkpoints").mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*60)
    print("   FRAMEWORK GRIDWORLD - EXEMPLES COMPLETS")
    print("="*60)
    
    try:
        example_1_basic_training()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 1: {e}")
    
    try:
        example_2_compare_algorithms()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 2: {e}")
    
    try:
        example_3_scalability_test()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 3: {e}")
    
    try:
        example_4_sensitivity_analysis()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 4: {e}")
    
    try:
        example_5_detailed_convergence_analysis()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 5: {e}")
    
    try:
        example_6_checkpoint_and_resume()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 6: {e}")
    
    try:
        example_7_stochastic_environment()
    except Exception as e:
        print(f"‚ùå Erreur dans exemple 7: {e}")
    
    print("\n" + "="*60)
    print("   TOUS LES EXEMPLES TERMIN√âS!")
    print("="*60)
    print("\nüìÅ R√©sultats sauvegard√©s dans:")
    print("   - results/plots/")
    print("   - results/checkpoints/")
    print("   - results/convergence_report.txt")


if __name__ == "__main__":
    # Vous pouvez ex√©cuter un exemple sp√©cifique:
    # example_1_basic_training()
    # example_2_compare_algorithms()
    # example_3_scalability_test()
    # example_4_sensitivity_analysis()
    # example_5_detailed_convergence_analysis()
    # example_6_checkpoint_and_resume()
    example_7_stochastic_environment()
    
    # Ou tous les exemples:
    # run_all_examples()