# ğŸ® GridWorld Reinforcement Learning Framework

Framework complet pour l'apprentissage et l'expÃ©rimentation d'algorithmes de Reinforcement Learning sur des environnements GridWorld.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-Compatible-green)](https://gymnasium.farama.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

## ğŸ“‹ Table des matiÃ¨res

- [CaractÃ©ristiques](#-caractÃ©ristiques)
- [Installation](#-installation)
- [Structure du projet](#-structure-du-projet)
- [DÃ©marrage rapide](#-dÃ©marrage-rapide)
- [Algorithmes implÃ©mentÃ©s](#-algorithmes-implÃ©mentÃ©s)
- [Analyses et visualisations](#-analyses-et-visualisations)
- [Exemples](#-exemples)
- [Documentation](#-documentation)

---

## âœ¨ CaractÃ©ristiques

### ğŸ¯ **Environnements**
- GridWorld 2D personnalisable
- Support des obstacles
- Environnements dÃ©terministes et stochastiques
- Compatible API Gymnasium
- Multiples modes de rendu

### ğŸ¤– **Algorithmes**
- **Programmation Dynamique**: Value Iteration, Policy Iteration
- **Monte Carlo**: First-Visit, Every-Visit
- **Temporal Difference**: Q-Learning, SARSA (Ã  venir)
- **AvancÃ©**: Double Q-Learning

### ğŸ“Š **Analyses**
- **Convergence**: Analyse dÃ©taillÃ©e de la convergence
- **ScalabilitÃ©**: Tests sur diffÃ©rentes tailles de grille
- **SensibilitÃ©**: Analyse des hyperparamÃ¨tres
- **Comparaison**: Benchmark entre algorithmes

### ğŸ’¾ **FonctionnalitÃ©s**
- Sauvegarde/chargement d'agents (checkpoints)
- Visualisations interactives
- Rapports automatiques
- Tracking complet des mÃ©triques

---

## ğŸš€ Installation

### PrÃ©requis
```bash
Python >= 3.8
```

### Installation des dÃ©pendances
```bash
pip install numpy matplotlib gymnasium
```

### Installation du framework
```bash
git clone https://github.com/Marc1T/gridworld-framework.git
cd gridworld_framework
pip install -e .
```

---

## ğŸ“ Structure du projet

```
gridworld_framework/
â”œâ”€â”€ core/                          # CÅ“ur du framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gridworld_env.py          # Environnement GridWorld
â”‚   â””â”€â”€ mdp.py                    # Classe MDP
â”‚
â”œâ”€â”€ agents/                        # Agents RL
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py             # Classe abstraite
â”‚   â”œâ”€â”€ random_agent.py           # Agent alÃ©atoire
â”‚   â”œâ”€â”€ value_iteration.py        # Value Iteration
â”‚   â”œâ”€â”€ policy_iteration.py       # Policy Iteration
â”‚   â”œâ”€â”€ monte_carlo.py            # Monte Carlo
â”‚   â””â”€â”€ q_learning.py             # Q-Learning
â”‚
â”œâ”€â”€ utils/                         # Utilitaires
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ visualization.py          # Visualisations
â”‚   â”œâ”€â”€ metrics.py                # MÃ©triques
â”‚   â””â”€â”€ convergence_analysis.py   # Analyses de convergence
â”‚
â”œâ”€â”€ examples/                      # Exemples
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ compare_algorithms.py
â”‚   â””â”€â”€ example_complete.py
â”‚
â”œâ”€â”€ tests/                         # Tests unitaires
â”‚   â”œâ”€â”€ test_env.py
â”‚   â””â”€â”€ test_agents.py
â”‚
â”œâ”€â”€ results/                       # RÃ©sultats (crÃ©Ã© automatiquement)
â”‚   â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ reports/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

---

## ğŸ® DÃ©marrage rapide

### 1. CrÃ©er un environnement

```python
from gridworld_framework.core.gridworld_env import GridWorldEnv

env = GridWorldEnv(
    grid_shape=(5, 5),        # Grille 5x5
    initial_state=0,          # DÃ©part en haut Ã  gauche
    goal_state=24,            # Objectif en bas Ã  droite
    obstacles=[7, 12, 13],    # Ã‰tats obstacles
    step_penalty=-0.01,       # PÃ©nalitÃ© par pas
    goal_reward=1.0           # RÃ©compense pour le but
)
```

### 2. EntraÃ®ner un agent

```python
from gridworld_framework.agents.q_learning import QLearningAgent

# CrÃ©er l'agent
agent = QLearningAgent(
    env,
    gamma=0.99,              # Facteur d'actualisation
    learning_rate=0.1,       # Taux d'apprentissage
    epsilon=1.0,             # Exploration initiale
    epsilon_decay=0.995      # DÃ©croissance de l'exploration
)

# EntraÃ®ner
history = agent.train(n_episodes=1000, verbose=True)

# Ã‰valuer
results = agent.evaluate(n_episodes=100)
print(f"Taux de succÃ¨s: {results['success_rate']:.1f}%")
```

### 3. Visualiser les rÃ©sultats

```python
from gridworld_framework.utils.visualization import (
    plot_learning_curve,
    plot_value_function,
    plot_policy
)

# Courbe d'apprentissage
plot_learning_curve(history)

# Fonction de valeur
plot_value_function(agent.V, env.grid_shape)

# Politique optimale
plot_policy(agent.policy, env.grid_shape, action_names=['â†‘','â†’','â†“','â†'])
```

### 4. Sauvegarder/charger

```python
# Sauvegarder
agent.save("checkpoints/my_agent.npz")

# Charger
agent.load("checkpoints/my_agent.npz")
```

---

## ğŸ¤– Algorithmes implÃ©mentÃ©s

### ğŸ“š Programmation Dynamique

#### **Value Iteration**
Calcule directement la fonction de valeur optimale.

```python
from gridworld_framework.agents.value_iteration import ValueIterationAgent

agent = ValueIterationAgent(env, gamma=0.99, theta=1e-6)
agent.train()
```

**Ã‰quation de Bellman optimale:**
```
V(s) = max_a Î£ P(s'|s,a)[R(s,a) + Î³V(s')]
```

#### **Policy Iteration**
Alterne entre Ã©valuation et amÃ©lioration de politique.

```python
from gridworld_framework.agents.policy_iteration import PolicyIterationAgent

agent = PolicyIterationAgent(env, gamma=0.99)
agent.train()
```

### ğŸ² Monte Carlo

Apprend Ã  partir d'Ã©pisodes complets.

```python
from gridworld_framework.agents.monte_carlo import MonteCarloAgent

agent = MonteCarloAgent(
    env, 
    gamma=0.99,
    epsilon=1.0,
    first_visit=True  # First-Visit ou Every-Visit
)
agent.train(n_episodes=5000)
```

### âš¡ Temporal Difference

#### **Q-Learning** (Off-Policy)
Apprend la fonction Q optimale directement.

```python
from gridworld_framework.agents.q_learning import QLearningAgent

agent = QLearningAgent(
    env,
    gamma=0.99,
    learning_rate=0.1,
    epsilon=1.0,
    use_double_q=False  # Double Q-Learning si True
)
agent.train(n_episodes=1000)
```

**Mise Ã  jour Q-Learning:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

---

## ğŸ“Š Analyses et visualisations

### 1. Analyse de convergence

```python
from gridworld_framework.utils.convergence_analysis import (
    analyze_convergence,
    generate_convergence_report
)

# Analyser
analysis = analyze_convergence(agent, metric='Q')
print(f"ConvergÃ©: {analysis['converged']}")
print(f"Ã‰pisode: {analysis['convergence_episode']}")

# Rapport dÃ©taillÃ©
report = generate_convergence_report(agent, save_path="report.txt")
print(report)
```

### 2. Test de scalabilitÃ©

```python
from gridworld_framework.utils.convergence_analysis import (
    test_gridworld_scalability,
    plot_scalability_results
)

# Tester sur diffÃ©rentes tailles
results = test_gridworld_scalability(
    agent_class=QLearningAgent,
    grid_sizes=[(4,4), (5,5), (6,6), (8,8), (10,10)],
    n_episodes=500,
    gamma=0.99
)

# Visualiser
plot_scalability_results(results)
```

### 3. Analyse de sensibilitÃ©

```python
from gridworld_framework.utils.convergence_analysis import (
    test_hyperparameter_sensitivity,
    plot_sensitivity_analysis
)

# Tester learning rates
results = test_hyperparameter_sensitivity(
    agent_class=QLearningAgent,
    env=env,
    param_name='learning_rate',
    param_values=[0.01, 0.05, 0.1, 0.3, 0.5, 0.9],
    n_episodes=500,
    n_trials=5
)

# Visualiser
plot_sensitivity_analysis(results, param_name='Learning Rate')
```

### 4. Comparaison d'algorithmes

```python
from gridworld_framework.utils.convergence_analysis import (
    plot_convergence_comparison
)

# EntraÃ®ner plusieurs agents
agents_histories = {
    'Value Iteration': vi_agent.get_history(),
    'Q-Learning': q_agent.get_history(),
    'Monte Carlo': mc_agent.get_history()
}

# Comparer
plot_convergence_comparison(agents_histories)
```

---

## ğŸ’¡ Exemples

### Exemple 1: Environnement simple

```python
# Grille 4x4 classique
env = GridWorldEnv(grid_shape=(4, 4))
agent = QLearningAgent(env)
agent.train(n_episodes=500)
agent.evaluate(n_episodes=100)
```

### Exemple 2: Avec obstacles

```python
# Grille avec obstacles
env = GridWorldEnv(
    grid_shape=(6, 6),
    obstacles=[8, 9, 14, 15, 20, 21]
)
agent = QLearningAgent(env)
agent.train(n_episodes=1000)
```

### Exemple 3: Environnement stochastique

```python
# 10% de bruit (actions alÃ©atoires)
env = GridWorldEnv(
    grid_shape=(5, 5),
    stochastic=True,
    noise=0.1
)
agent = QLearningAgent(env, learning_rate=0.1)
agent.train(n_episodes=2000)
```

### Exemple 4: Benchmark complet

Voir `examples/example_complete.py` pour un exemple complet incluant:
- EntraÃ®nement de plusieurs algorithmes
- Tests de scalabilitÃ©
- Analyses de sensibilitÃ©
- Visualisations complÃ¨tes
- Sauvegarde des rÃ©sultats

```bash
python examples/example_complete.py
```

---

## ğŸ“– Documentation

### Classes principales

#### **GridWorldEnv**
```python
GridWorldEnv(
    grid_shape=(4, 4),           # Dimensions
    initial_state=0,             # Ã‰tat de dÃ©part
    goal_state=15,               # But
    obstacles=[],                # Liste d'obstacles
    stochastic=False,            # Transitions stochastiques
    noise=0.0,                   # Niveau de bruit
    step_penalty=-0.01,          # CoÃ»t par action
    goal_reward=1.0,             # RÃ©compense du but
    obstacle_penalty=-1.0,       # PÃ©nalitÃ© obstacle
    render_mode=None             # 'human', 'matplotlib', 'rgb_array'
)
```

#### **BaseAgent**
MÃ©thodes communes Ã  tous les agents:
- `train(n_episodes, max_steps, verbose)`
- `evaluate(n_episodes, render)`
- `act(state, explore)`
- `save(filepath)`
- `load(filepath)`
- `get_policy()`
- `get_value_function()`
- `get_q_function()`

---

## ğŸ”¬ Analyses disponibles

### MÃ©triques de performance
- RÃ©compense moyenne et Ã©cart-type
- Taux de succÃ¨s
- Longueur moyenne des Ã©pisodes
- Vitesse de convergence
- StabilitÃ© post-convergence

### Visualisations
- Courbes d'apprentissage
- Fonctions de valeur (V et Q)
- Politiques optimales
- Comparaisons entre algorithmes
- Tests de scalabilitÃ©
- Analyses de sensibilitÃ©

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer de nouvelles fonctionnalitÃ©s
- AmÃ©liorer la documentation
- Ajouter des exemples

N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Ouvrir une Pull Request
   
---

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---
## ğŸ™ Remerciements

- InspirÃ© par [Gymnasium](https://gymnasium.farama.org/)
- BasÃ© sur les principes de [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) de Sutton et Barto

---

**Bon apprentissage ! ğŸš€**

---