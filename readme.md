# GridWorld Reinforcement Learning Framework

Un framework modulaire pour l'apprentissage par renforcement basÃ© sur des environnements GridWorld, inspirÃ© de Gymnasium.

## ðŸš€ FonctionnalitÃ©s

- **Environnement GridWorld** compatible avec l'API Gymnasium
- **Multiples algorithmes RL** implÃ©mentÃ©s :
  - ðŸŽ¯ Policy Iteration
  - ðŸ’° Value Iteration  
  - ðŸŽ² Monte Carlo
  - ðŸ¤– Q-learning
  - ðŸŽª Agent alÃ©atoire (baseline)
- **Visualisations complÃ¨tes** :
  - Courbes d'apprentissage
  - Fonctions de valeur V(s) et Q(s,a)
  - Politiques optimales
  - Ã‰tats du GridWorld en temps rÃ©el
- **Interface conviviale** pour configurer les environnements
- **Structure modulaire** et extensible

## ðŸ“¦ Installation

### Installation directe

```bash
git clone https://github.com/votre-username/gridworld-rl-framework.git
cd gridworld-rl-framework
pip install -e .
```

### DÃ©pendances

```bash
pip install numpy matplotlib seaborn gymnasium
```

## ðŸŽ¯ Utilisation rapide

### Exemple basique

```python
from gridworld_framework import GridWorldEnv, QLearningAgent
from gridworld_framework.utils.visualization import plot_learning_curve

# CrÃ©er l'environnement
env = GridWorldEnv(
    grid_shape=(4, 4),
    initial_state=0,
    goal_state=15,
    obstacles=[5, 7, 11],
    render_mode='human'
)

# CrÃ©er et entraÃ®ner l'agent
agent = QLearningAgent(env, gamma=0.99, learning_rate=0.1)
history = agent.train(n_episodes=1000)

# Visualiser les rÃ©sultats
plot_learning_curve(history)
```

### ExÃ©cuter les dÃ©monstrations

```python
from gridworld_framework.examples import compare_all_agents

# Comparer tous les algorithmes
env, agents, results = compare_all_agents()
```

## ðŸ“š Algorithmes implÃ©mentÃ©s

### ðŸŽ¯ Policy Iteration
Algorithme de planification qui alterne entre l'Ã©valuation et l'amÃ©lioration de politique.

```python
from gridworld_framework import PolicyIterationAgent

agent = PolicyIterationAgent(env, gamma=0.99, theta=1e-6)
agent.train()
```

### ðŸ’° Value Iteration  
Algorithme de planification qui calcule directement la fonction de valeur optimale.

```python
from gridworld_framework import ValueIterationAgent

agent = ValueIterationAgent(env, gamma=0.99, theta=1e-6)
agent.train()
```

### ðŸŽ² Monte Carlo
MÃ©thode d'apprentissage sans modÃ¨le basÃ©e sur des Ã©pisodes complets.

```python
from gridworld_framework import MonteCarloAgent

agent = MonteCarloAgent(
    env, 
    gamma=0.99,
    epsilon=1.0,
    epsilon_decay=0.995
)
agent.train(n_episodes=1000)
```

### ðŸ¤– Q-learning
Algorithme TD off-policy populaire pour l'apprentissage sans modÃ¨le.

```python
from gridworld_framework import QLearningAgent

agent = QLearningAgent(
    env,
    gamma=0.99,
    learning_rate=0.1,
    epsilon=1.0,
    epsilon_decay=0.998
)
agent.train(n_episodes=1000)
```

## ðŸŽ¨ Visualisation

### Courbes d'apprentissage
```python
from gridworld_framework.utils.visualization import plot_learning_curve

history = agent.train(n_episodes=1000)
plot_learning_curve(history)
```

### Fonction de valeur et politique
```python
from gridworld_framework.utils.visualization import (
    plot_value_function,
    plot_policy,
    plot_q_function
)

# Fonction de valeur V(s)
plot_value_function(agent.get_value_function(), env.grid_shape)

# Politique optimale
plot_policy(agent.get_policy(), env.grid_shape)

# Fonction Q(s,a)
plot_q_function(agent.get_q_function(), env.grid_shape)
```

### Visualisation complÃ¨te
```python
from gridworld_framework.utils.visualization import visualize_gridworld

visualize_gridworld(env, agent)
```

## âš™ï¸ Configuration de l'environnement

### Grille personnalisÃ©e
```python
env = GridWorldEnv(
    grid_shape=(5, 5),           # Taille de la grille
    initial_state=0,             # Ã‰tat initial
    goal_state=24,               # Ã‰tat objectif
    goal_fixed=True,             # Objectif fixe ou alÃ©atoire
    obstacles=[6, 12, 18],       # Ã‰tats obstacles
    render_mode='matplotlib'     # Mode de rendu
)
```

### Matrices de transition personnalisÃ©es
```python
# AccÃ©der aux matrices MDP
P = env.get_transition_matrix()  # P[s, a, s']
R = env.get_reward_matrix()      # R[s, a]

# Modifier les matrices si nÃ©cessaire
P[0, 0, 1] = 1.0  # Transition certaine
```

## ðŸ“Š MÃ©triques et Ã©valuation

### Ã‰valuation simple
```python
results = agent.evaluate(n_episodes=100)
print(f"Reward moyen: {results['mean_reward']:.2f}")
```

### Comparaison d'agents
```python
from gridworld_framework.utils.metrics import compare_agents

agents = {
    'Q-learning': q_agent,
    'Monte Carlo': mc_agent,
    'Policy Iteration': pi_agent
}

results = compare_agents(agents, env, n_episodes=100)
```

## ðŸ—ï¸ Structure du projet

```
gridworld_framework/
â”œâ”€â”€ core/                 # CÅ“ur du framework
â”‚   â”œâ”€â”€ gridworld_env.py  # Environnement GridWorld
â”‚   â””â”€â”€ mdp.py           # Classe MDP
â”œâ”€â”€ agents/              # ImplÃ©mentations des algorithmes
â”‚   â”œâ”€â”€ base_agent.py    # Classe de base
â”‚   â”œâ”€â”€ random_agent.py
â”‚   â”œâ”€â”€ policy_iteration.py
â”‚   â”œâ”€â”€ value_iteration.py
â”‚   â”œâ”€â”€ monte_carlo.py
â”‚   â””â”€â”€ q_learning.py
â”œâ”€â”€ utils/               # Utilitaires
â”‚   â”œâ”€â”€ visualization.py # Fonctions de visualisation
â”‚   â””â”€â”€ metrics.py       # MÃ©triques et comparaisons
â”œâ”€â”€ examples/            # Exemples d'utilisation
â”‚   â””â”€â”€ basic_usage.py
â””â”€â”€ tests/               # Tests unitaires
```

## ðŸ”§ Extension du framework

### Ajouter un nouvel agent
```python
from gridworld_framework.agents.base_agent import BaseAgent

class MyCustomAgent(BaseAgent):
    def __init__(self, env, **kwargs):
        super().__init__(env, **kwargs)
        # Initialisation personnalisÃ©e
    
    def act(self, state, explore=True):
        # ImplÃ©mentation de la sÃ©lection d'action
        pass
    
    def update(self, state, action, reward, next_state, done):
        # ImplÃ©mentation de la mise Ã  jour
        pass
```

### CrÃ©er un environnement personnalisÃ©
```python
from gridworld_framework.core.gridworld_env import GridWorldEnv

class MyCustomGridWorld(GridWorldEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Personnalisations
    
    def _setup_mdp(self):
        # Logique MDP personnalisÃ©e
        pass
```

## ðŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Ouvrir une Pull Request

## ðŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ðŸ™ Remerciements

- InspirÃ© par [Gymnasium](https://gymnasium.farama.org/)
- BasÃ© sur les principes de [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) de Sutton et Barto
```

## 15. Fichier requirements.txt

**requirements.txt**
```txt
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.11.0
gymnasium>=0.28.0
```

## RÃ©sumÃ© final

Voici la structure complÃ¨te que nous avons crÃ©Ã©e :

```
gridworld_framework/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gridworld_env.py
â”‚   â””â”€â”€ mdp.py
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”œâ”€â”€ random_agent.py
â”‚   â”œâ”€â”€ policy_iteration.py
â”‚   â”œâ”€â”€ value_iteration.py
â”‚   â”œâ”€â”€ monte_carlo.py
â”‚   â””â”€â”€ q_learning.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ basic_usage.py
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Comment tester le framework :

1. **Installation** :
```bash
pip install -e .
```

2. **Test basique** :
```python
from gridworld_framework.examples import demo_random_agent
env, agent = demo_random_agent()
```

3. **Test complet** :
```python
from gridworld_framework.examples import compare_all_agents
env, agents, results = compare_all_agents()
```
